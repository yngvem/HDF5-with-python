{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to HDF5 and Python\n",
    "\n",
    "This notebook shows my exploration of the HDF5 format for saving data. It is based by the book *Python and HDF5* by Andrew Collette.\n",
    "\n",
    "The benefits of using a HDF5 fileformat is that it acts similar to a Numpy array, but the data lies on disk till it is requested. This makes it easy to deal with datasets so large that they do not fit in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our first HDF5 datafile\n",
    "\n",
    "We create and load HDF5 files by creating instances of the HDF5 File class.\n",
    "\n",
    "To create an empty file (and overwrite any existing files with the same name) write the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'our_first_datafile.hdf5'\n",
    "with h5py.File(filename, 'w') as hdf_file:\n",
    "    # Code manipulating the datafile\n",
    "    pass\n",
    "# The datafile is now closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created an empty HDF5 file in our current directory. The name of that file is *our_first_datafile.hdf5*. The *'w'* argument for this function symbolise that we want to create a new, empty file and overwrite any existing files.\n",
    "\n",
    "There are several flags we can supply the HDF5 initialiser function, to do different things, they are\n",
    "\n",
    "| Flag | Meaning                                                                       |\n",
    "|:---- |:----------------------------------------------------------------------------- |\n",
    "| 'w'  | Create new file and overwrite existing files                                  |\n",
    "| 'w-' | Create new empty file but raise an error if it already exists                 |\n",
    "| 'r'  | Open file in read only mode and raise error if the file doesn't exist         |\n",
    "| 'r+' | Open file in read and write mode and raise error if the file doesn't exist    |\n",
    "| 'a'  | Open file in read and write mode and create an empty file if it doesn't exist |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating and reading the data in our datafile.\n",
    "\n",
    "The benefit of HDF5 files is that they allow for a structured handling of data on disk, but in a way that is efficient for reading and writing. Data is stored in *datasets*, which is the HDF5 equivalent of a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09125845 0.09709407]\n",
      " [0.59468209 0.63913843]\n",
      " [0.62462814 0.05412021]]\n"
     ]
    }
   ],
   "source": [
    "data = np.random.random((3, 2))\n",
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    hdf_file['dataset_1'] = data\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "What we just did was to create a dataset of the same shape and type as our numpy array, `data` and store the content of `data` in it. Note that if we try to overwrite an existing dataset this way, we will get an error message. If we want to change the content of a dataset, we have to do that through slicing. To change everything, we can write this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99742839 0.66538952]\n",
      " [0.47259286 0.94203899]\n",
      " [0.89594797 0.59381092]]\n"
     ]
    }
   ],
   "source": [
    "data = np.random.random((3, 2))\n",
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    hdf_file['dataset_1'][...] = data\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We cannot directly look at the contents of a dataset, if we try to print it, we get this message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"dataset_1\": shape (3, 2), type \"<f8\">\n",
      "<Closed HDF5 dataset>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    dataset = hdf_file['dataset_1']\n",
    "    print(dataset)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The reason for that is that the content of a datafile is only loaded to memory on-demand, so  you have to explicitly ask for the data you want to see. This can be done through a simple slicing. Here are some examples of this in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some data:\n",
      "[[0.94203899]\n",
      " [0.59381092]]\n",
      "All data\n",
      "[[0.99742839 0.66538952]\n",
      " [0.47259286 0.94203899]\n",
      " [0.89594797 0.59381092]]\n",
      "Is the data loaded from the datafile the same as the data we last inserted? True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    some_data = hdf_file['dataset_1'][1:3, 1:2]\n",
    "    all_data = hdf_file['dataset_1'][...]\n",
    "\n",
    "print('Some data:')\n",
    "print(some_data)\n",
    "print('All data')\n",
    "print(all_data)\n",
    "\n",
    "print('Is the data loaded from the datafile the same as the data we last inserted?', np.array_equal(data, all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "There are several ways to insert datasets in a datafile. For example, if we know that we want to insert a 10000x10000 matrix in the datafile, then we tell the HDF5 file that we will insert that much data, but the storage space will be allocated as we insert data.\n",
    "\n",
    "We can also control what type the dataset should take (say 16 bit float instead of 64 bit). The type casting will then be performed automatically by the HDF5 software while it is writing the data to disk. \n",
    "\n",
    "A better way of creating new datasets is through the create_dataset function, which gives us more control over the structure of the dataset. Below is an example of this in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[1.+0.j 1.+0.j 1.+0.j]\n",
      " [1.+0.j 1.+0.j 1.+0.j]\n",
      " [1.+0.j 1.+0.j 1.+0.j]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    hdf_file.create_dataset('empty_dataset1', shape=(3, 1))\n",
    "    hdf_file.create_dataset('empty_dataset2', shape=(10, 4), dtype=np.float16)\n",
    "    hdf_file.create_dataset('ones_dataset1', data=np.ones((3, 3), dtype=np.complex64))\n",
    "    \n",
    "    dataset1 = hdf_file['empty_dataset1'][...]\n",
    "    dataset2 = hdf_file['empty_dataset2'][...]\n",
    "    dataset3 = hdf_file['ones_dataset1'][...]\n",
    "    \n",
    "print(dataset1)\n",
    "print(dataset2)\n",
    "print(dataset3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note that HDF5 will not warn before overflow errors during type-casting. To demonstrate this, see the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.689349e+19]\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    hdf_file['empty_dataset1'][1] = 2.**65\n",
    "    print(hdf_file['empty_dataset1'][1])\n",
    "    \n",
    "    hdf_file['empty_dataset2'][1, 1] = 2.**65\n",
    "    print(hdf_file['empty_dataset2'][1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notice how the value of the float16 dataset simply was set to infinity? This is a *gotcha* to be aware of when using limited capacity numericals in HDF5 files.\n",
    "\n",
    "We have now created a datafile and demonstrated how values are typecasted to the appropriate type when they are written into a datafile. Now we want to do this typecasting at read-time. There are several ways to do this, and here are two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16\n",
      "[[ 0.]\n",
      " [inf]\n",
      " [ 0.]]\n",
      "float16\n",
      "[[ 0.]\n",
      " [inf]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    dataset = hdf_file['empty_dataset1']\n",
    "    \n",
    "    # Method 1\n",
    "    out1 = np.empty(dataset.shape, dtype=np.float16)\n",
    "    dataset.read_direct(out1)\n",
    "    \n",
    "    # Method 2\n",
    "    with dataset.astype('float16'):\n",
    "        out2 = dataset[...]\n",
    "        \n",
    "print(out1.dtype)\n",
    "print(out1)\n",
    "print(out2.dtype)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notice here, that once again the value(s) outside the numerical range of the variable we cast to are set to infinity without warning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "\n",
    "An integral part of the python framework is slicing. Both Numpy and Pandas have powerful slicing capabilities, and so does HDF5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data:\n",
      "[[1 9 5 3 0 3 6 0 0 6]\n",
      " [7 8 3 8 1 4 1 1 7 8]\n",
      " [6 4 0 8 3 0 1 5 1 1]\n",
      " [2 6 3 8 8 8 6 9 2 1]\n",
      " [2 1 7 1 2 3 1 2 6 2]]\n",
      "A row of data:\n",
      "[[7 8 3 8 1 4 1 1 7 8]]\n",
      "A column of data:\n",
      "[[5]\n",
      " [3]\n",
      " [0]\n",
      " [3]\n",
      " [7]]\n",
      "A matrix of data:\n",
      "[[7 8 3]\n",
      " [6 4 0]\n",
      " [2 6 3]\n",
      " [2 1 7]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'a') as hdf_file:\n",
    "    dataset = hdf_file.create_dataset('random', data=np.random.randint(0, 10, (5, 10)))\n",
    "    \n",
    "    # Get all data\n",
    "    data = dataset[...] # [...] is the same as [:, :]\n",
    "    \n",
    "    # Get all data in the second row\n",
    "    some_row = dataset[1:2, :]\n",
    "    \n",
    "    # Get all data in the third column\n",
    "    some_column = dataset[:, 2:3]\n",
    "    \n",
    "    # Extract a matrix starting at the first row, ending at the third\n",
    "    # and starting at the second column, ending at the fifth\n",
    "    some_matrix = dataset[1:5, 0:3]\n",
    "    \n",
    "\n",
    "    \n",
    "print('All data:')\n",
    "print(data)\n",
    "print('A row of data:')\n",
    "print(some_row)\n",
    "print('A column of data:')\n",
    "print(some_column)\n",
    "print('A matrix of data:')\n",
    "print(some_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It is important to note that boolean slicing is accepted, however, we cannot perform them as simply as with a numpy array. The following code shows this by raising an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'Dataset' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-46cda19b51bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Try a simple bool-slicing to get numbers greater than 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mgt5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The numbers in our random dataset that are greater than five are'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'Dataset' and 'int'"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file['random']\n",
    "    \n",
    "    # Try a simple bool-slicing to get numbers greater than 5\n",
    "    gt5 = dataset[dataset > 5]\n",
    "    \n",
    "print('The numbers in our random dataset that are greater than five are')\n",
    "print(gt5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If you want to use boolean slicing, you need to directly supply the boolean mask that you want to use, as the HDF5 dataset does not allow for direct mathematical operations like Numpy arrays do.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note, optimizing the read speed.\n",
    "\n",
    "We have here used slicing directly to get the data, instead of first allocating memory using ```np.empty``` and then filling it with ```dataset.read_direct```. The direct slicing notation is more readable, but the read_direct command is faster. There are some internal HDF5 reasons for this, but I did not bother to spend time understanding why. If we want to slice using this approach, we can do that the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7. 8. 3. 8. 1. 4. 1. 1. 7. 8.]\n",
      " [2. 6. 3. 8. 8. 8. 6. 9. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file['random']\n",
    "    data = np.empty((2, dataset.shape[1]))\n",
    "    \n",
    "    dataset.read_direct(data, np.s_[1:5:2, :])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing datasets\n",
    "One important feature of any datasaving system is the option to extend it easily. HDF5 supports this, however it requires that this was enabled when the dataset was created. Here we demonstrate that it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only chunked datasets can be resized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d3104c67c171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhdf_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'random'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\NMBU\\Miniconda3\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, axis)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only chunked datasets can be resized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Only chunked datasets can be resized"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file['random']\n",
    "    dataset.resize((np.prod(dataset.shape), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us now create a new dataset which we can reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file.create_dataset(name='reshapable', shape=(2, 2), \n",
    "                                      dtype='float32', maxshape=(None, None))\n",
    "    print(dataset[...])\n",
    "    dataset.resize((4, 1))\n",
    "    print(dataset[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The ```maxshape``` argument is needed if one want a dataset to be resizable. It (obviously) specifies the maximum size of the dataset. If we, however, do not know the maximum length of an axis, we can write None. This simply means that there is no maximum length of that axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful when reshaping.\n",
    "\n",
    "In Numpy, reshaping arrays will shuffle the elements around, as demonstrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "a = a.reshape((2, 2))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we reshape a dataset, this is not the behaviour we get. as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping\n",
      " [[2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "After reshaping \n",
      " [[2. 0.]\n",
      " [3. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file['reshapable']\n",
    "    dataset[...] = np.arange(2, 6).reshape((4, 1))\n",
    "    print('Before reshaping\\n', dataset[...])\n",
    "    dataset.resize((2, 2))\n",
    "    print('After reshaping \\n', dataset[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It is clear that one should be careful when reshaping. Note that the first two values were kept. this is all a result of hw HDF5 internally deals with resizing datasets. Luckily, this is not an issue when extending datasets, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding row\n",
      " [[2. 3.]\n",
      " [4. 5.]]\n",
      "After adding row \n",
      " [[2. 3. 0.]\n",
      " [4. 5. 0.]]\n",
      "--------------------------------\n",
      "Filling empty row\n",
      "--------------------------------\n",
      "Before adding column\n",
      " [[2. 3. 4.]\n",
      " [5. 6. 7.]]\n",
      "After adding column\n",
      " [[2. 3. 4.]\n",
      " [5. 6. 7.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file['reshapable']\n",
    "    dataset[...] = np.arange(2, 6).reshape((2, 2))\n",
    "    print('Before adding row\\n', dataset[...])\n",
    "    dataset.resize((2, 3))\n",
    "    print('After adding row \\n', dataset[...])\n",
    "    print('--------------------------------')\n",
    "    print('Filling empty row')\n",
    "    print('--------------------------------')\n",
    "    dataset[...] = np.arange(2, 8).reshape((2, 3))\n",
    "    print('Before adding column\\n', dataset[...])\n",
    "    dataset.resize((3, 3))\n",
    "    print('After adding column\\n', dataset[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping\n",
    "\n",
    "One integral part of the HDF file format is grouping. It can be considered as a folder structure within the datafile. A group might contain several subgroups and datasets. This gives a very orderly way of dealing with data. Below is a simple example where we create a group and a dataset within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    group = hdf_file.create_group('some_group')\n",
    "    dataset1 = group.create_dataset(name='first_ordered_dataset', shape=(3, 3),\n",
    "                                    dtype='float32')\n",
    "    group['second_ordered_dataset'] = np.ones((2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to view the data in one of the datasets, we can do it these ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Second data \n",
      " [[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    # First way\n",
    "    data1 = hdf_file['some_group/first_ordered_dataset'][...]\n",
    "    \n",
    "    # Second way\n",
    "    group = hdf_file['some_group']\n",
    "    data2 = group['second_ordered_dataset'][...]\n",
    "\n",
    "print('First data \\n', data1)\n",
    "print('Second data \\n', data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can also create subgroups. Below are some examples where we do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    # Creating a single subgroup in existing group\n",
    "    subgroup1 = hdf_file['some_group'].create_group('some_subgroup')\n",
    "    \n",
    "    # Creating a several nested subgroups at once\n",
    "    subgroup2 = hdf_file.create_group('a_group/a_subgroup/a_subsubgroup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the line ```subgroup2 = hdf_file.create_group('a_group/a_subgroup/a_subsubgroup')``` first created the group ```a_group```, it then created a subgroup named ```a_subgroup``` within ```a_group``` before it finally created the subgroup ```a_subsubgroup``` within ```a_subgroup```. The variable ```subgroup2``` became the innermost group (```a_subsubgroup```).\n",
    "\n",
    "We can also create groups at the same time as we create a dataset. The code below demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    # Creating a single subgroup in existing group\n",
    "    subgroup1 = hdf_file.create_dataset(name='new_group/new_dataset', shape=(3, 3),\n",
    "                                        dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to view the content of this dataset, we can simply write the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    data = hdf_file['new_group/new_dataset'][...]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over a datafile\n",
    "We have now almost reached the end of the notes. Before we head on to the final chapter, we quickly go through how we iterate over the content of a HDF5 file. Iterating over a HDF5 file in Python works similarly to how we iterate over dictionaries, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_group\n",
      "dataset_1\n",
      "empty_dataset1\n",
      "empty_dataset2\n",
      "new_group\n",
      "ones_dataset1\n",
      "random\n",
      "reshapable\n",
      "some_group\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    for i in hdf_file:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As you see, we iterate over the entries that we can use when iterating over a HDF5 file. If we want to iterate over the groups within a HDF5 file, we can do that this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_group\n",
      "new_group\n",
      "some_group\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    for i in hdf_file:\n",
    "        if isinstance(hdf_file[i], h5py.Dataset):\n",
    "            continue  # Don't show dataset names\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can also iterate over the entries of a group the exact same way as we iterate over the entries of a File (in fact, the File class is a subclass of the Group class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_subgroup\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    for i in hdf_file['some_group']:\n",
    "        if isinstance(hdf_file['some_group'][i], h5py.Dataset):\n",
    "            continue  # Don't show dataset names\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking - A method to optimize read-time\n",
    "\n",
    "It is helpful to imagine a HDD (or SSD) and RAM as strips of paper where numbers representing our data is written one after another. When we want to read a file, it is quicker if the data is stored physically close inside the computer, instead of spread out. Below is a figure that demonstrate how data can be stored as a contiguous block or as several fragments.\n",
    "\n",
    "![Figure showing data stored as a contiguous chunk and data stored as several fragments](https://notebooks.azure.com/yngvemoe/libraries/masternotebooks/raw/HDF5%2Ffigures%2Fdata_on_disk.png)\n",
    "\n",
    "Here we come to an interesting problem, images are two dimensional constructs, add channels and they become three dimensional. The moment we have a series of colour images, we have a four dimensional construct that we want to store as a one dimensional list. To do this, we obviously have to \"unwrap\" the data, and the way we do that can severly impare read speed.\n",
    "\n",
    "Imagine, for example, if the red-value of the top-left pixel of the first image is stored next to the red-value of the top-left pixel of the second image. This will make it fast to load the red-values of all the top-left pixels at the same time. However, loading a full image will take time, as the pixel just below the top-left pixel is separated from the value of the top-left pixel.\n",
    "\n",
    "Luckily, HDF5 allows us to manually say how the pixels should be stored. This is called chunking. I will not elaborate much here, just enough to get an efficient pipeline to load 2D images. For an excellent introduction to this topic (and compression of HDF5 files) I reccomend reading the book *Python and HDF5* by Andrew Collette.\n",
    "\n",
    "---\n",
    "Data in HDF5 is by default stored so that the values of the last axis is physically adjacent to each other. The line ```data = dataset[1, 1, :]``` will take shorter time than the line ```data = dataset[1, :, 1]```, which again will take shorter time than the line ```data = dataset[:, 1, 1]```.\n",
    "\n",
    "Imagine now that we have images stored on the format \\[image_number, height, width, channel\\]. This will make it fast to load the colour values of a single pixel, which might be exactly what we want. However, we might also be interested in it being faster to get the colour values of a single channel in an image, rather than to get all the colours of a single pixel. To do this, we create the dataset in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(filename) as hdf_file:\n",
    "    dataset = hdf_file.create_dataset(name='chunking', shape=(4, 10, 10, 3), \n",
    "                                      maxshape=(None, 10, 10, 3), \n",
    "                                      chunks=(1, 10, 10, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did here was to create a dataset called *chunking*, consisting of 4 10-by-10 RGB images. We specified that we can add new images, but not change the shape or number of colour bands of the images. We also specified that we want the second and third stored as close as possible on disk.\n",
    "\n",
    "Making sure that data is stored this way is significantly more important when data is stored on a HDD rather than on an SSD. This is because on a HDD, the pin reading the disk has to physically move a lot when reading fragmented data. This is not the case in a solid state drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding notes\n",
    "There are other important aspects of the HDF5 file type such as how to attach metadata and attributes to datasets and groups. However, those will not be covered in these notes for now (but might be added later). \n",
    "\n",
    "If these notes caught your attention, I very much reccomend buying the book *Python and HDF5* it is a clearly written book, with easy to understand examples. It also goes further into implementation details but not further than anyone that simply use HDF5 as a tool in their data pipeline needs to understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {
    "height": "238px",
    "width": "353px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
